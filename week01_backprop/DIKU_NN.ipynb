{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.37829184]]), array([[ 0.37829185]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the alternative sigmoid derivative\n",
    "def f_sig(a):\n",
    "    return a/(1+abs(a))\n",
    "\n",
    "def df_sig(a):\n",
    "    return 1/(1+abs(a))**2\n",
    "\n",
    "h = 0.00000001\n",
    "x = np.random.random((1,1))\n",
    "(f_sig(x+h) - f_sig(x)) / h, df_sig(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the approach is using vectors/matrices, \n",
    "# thus all inputs are applied at once\n",
    "X = np.random.random((2,3))\n",
    "def initialise_weights(num_neurons):\n",
    "    # the weights are between -1 and 1, with mean 0\n",
    "    w_b0_i = 2*np.random.random((num_neurons, 1)) - 1\n",
    "    w0_i = 2*np.random.random((num_neurons, 1)) - 1\n",
    "\n",
    "    w_b1_i = 2*np.random.random((1, 1)) - 1\n",
    "    w1_i = 2*np.random.random((num_neurons, 1)) - 1 \n",
    "    return w_b0_i, w0_i, w_b1_i, w1_i \n",
    "\n",
    "def feed_forward(w_b0, w0, w_b1, w1):\n",
    "    # z0 is the output of the hidden layer\n",
    "    a0 = np.dot(w0, X) + w_b0\n",
    "    z0 = f_sig(a0)\n",
    "\n",
    "    # z1 is the final output (predictions/y_hat)   \n",
    "    a1 = np.dot(z0.T, w1) + w_b1\n",
    "    z1 = a1\n",
    "    return (a0, z0, a1, z1)\n",
    "\n",
    "\n",
    "w_b0, w0, w_b1, w1 = initialise_weights(4)\n",
    "a0, z0, a1, z1 = feed_forward(w_b0, w0, w_b1, w1)\n",
    "\n",
    "mse(z1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(z0, a1, z1):\n",
    "    d_output = z1 - y\n",
    "    dw1 = np.dot(dZ2, A1.T)\n",
    "    np.sum(dZ2, axis=1)\n",
    "    \n",
    "    deriv_layer = np.multiply(np.dot(W2.T, dZ2), d_alternative_sigmoid(Z1))\n",
    "    np.dot(dZ1, X.T)\n",
    "    np.sum(dZ1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 7), (2, 25))"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 1\n",
    "K = 1\n",
    "M = 4\n",
    "e\n",
    "# invariant : j<i\n",
    "Wij = np.zeros((d+M+K+1,d+M+K+1))\n",
    "Wij.shape,Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1105e24e0>"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1145e35f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# random init biases for both hidden layer and output\n",
    "Wij[d+1:d+1+M+K,0:1] = 2*np.random.random((M+K,1))-1\n",
    "# random init weights for the hidden layer\n",
    "Wij[d+1:d+M+1,1:d+1] = 2*np.random.random((M,d))-1\n",
    "# random init weights for the output layer\n",
    "Wij[d+M+K:d+M+K+1,d+1:d+1+M] = 2*np.random.random((K,M))-1\n",
    "plt.spy(Wij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41821277]] [[-0.108044]] [[ 0.52625677]]\n",
      "0.27694619062\n"
     ]
    }
   ],
   "source": [
    "#add the bias to the input\n",
    "Xtr = np.vstack([X,np.ones((1,25))])\n",
    "ytr = y\n",
    "\n",
    "# forward pass :\n",
    "def mse(yh, y):\n",
    "    return np.mean((y - yh)**2)\n",
    "\n",
    "def sig(a):\n",
    "    return 1/(1 + np.abs(a))\n",
    "ybatch=ytr[:,0:n]\n",
    "Xbatch=Xtr[:,0:n]\n",
    "n=1\n",
    "A=np.zeros((M,n))\n",
    "Z=np.zeros((M,n))\n",
    "# for each of the neurons in the hidden layer\n",
    "ii=-1\n",
    "for i in range(d+1,d+1+M):\n",
    "    ii = ii + 1 \n",
    "#     print(i,Wij[i,0:d+1])\n",
    "#     Xtr[:,0:1],Wij[i,0:d+1],Wij[i,0:d+1].dot(Xtr[:,0:1])\n",
    "    a = Wij[i,0:d+1].dot(Xbatch)\n",
    "    z = sig(a)\n",
    "#     print((a,z))\n",
    "    A[ii,:] = a\n",
    "    Z[ii,:] = z\n",
    "# print((A,Z))\n",
    "\n",
    "# add the bias to the hidden layer output:\n",
    "Z = np.vstack([Z,np.ones((1,n))])\n",
    "# compute the output value:\n",
    "yhat = np.zeros((K,n))\n",
    "ii=-1\n",
    "for i in range(d+1+M,d+1+M+K):\n",
    "    ii = ii+1\n",
    "    w_idx = np.r_[[0],d+1:d+1+M]\n",
    "    w = Wij[i:i+1,w_idx]\n",
    "#     print(w,A,w.dot(A))\n",
    "    yhat[ii,:] = w.dot(Z)\n",
    "print(yhat,ybatch,yhat-ybatch)\n",
    "print(mse(yhat,ybatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.52625677]]\n",
      "[[-0.52625677]]\n",
      "[[-0.52625677]]\n",
      "[[-0.52625677]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x114a7f4a8>"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQOXGjNwqTEDlxr7P0g3zwp4mmYFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114603f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# backward pass:\n",
    "\n",
    "def d_mse(yh, y):\n",
    "    return y - yh\n",
    "\n",
    "def d_sig(a):\n",
    "    return 1/(1+abs(a))**2\n",
    "\n",
    "# Create a matrix of same shape as Wij to hold gradients\n",
    "dWij = np.zeros((d+M+K+1,d+M+K+1))\n",
    "\n",
    "# compute gradient from output layer:\n",
    "ii=K\n",
    "for i in range(d+M+K,d+M,-1):\n",
    "    ii = ii-1\n",
    "    E = d_mse(yhat,ybatch)\n",
    "    # distribute the MSE according to the weights of the output layer\n",
    "    w_idx = np.r_[[0],d+1:d+1+M]\n",
    "    w = Wij[i:i+1,w_idx]\n",
    "    # there exists an error from every point\n",
    "#     print(\"error for each point: \",E,\"\\n weights: \",w,\"\\n number of points:\",E.shape[1]) \n",
    "    # gradient of error from each of the points\n",
    "    dE_X = 1/E.shape[1]*w.T*E\n",
    "    # gradient of the error aggregated on every dimension\n",
    "#     print(np.sum(dE_X,axis=1))\n",
    "    dE_d = 1/len(w_idx)*np.sum(dE_X,axis=1)\n",
    "#     print(\"w and dE are linearly dependent; dE:\",dE_d,\"w:\",w,\"w/dE:\", w/dE_d)\n",
    "    dWij[i:i+1,w_idx]=dE_d\n",
    "    \n",
    "\n",
    "\n",
    "# compute gradients of neurons in the hidden layer in reverse order\n",
    "ii=M\n",
    "for i in range(d+1+M,d+1,-1):\n",
    "    E=d_mse(yhat,ybatch)\n",
    "    print(E)\n",
    "    ii = ii - 1\n",
    "    a = A[ii,:]\n",
    "    z = Z[ii,:]\n",
    "    \n",
    "plt.spy(dWij)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
