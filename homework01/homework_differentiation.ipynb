{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
    "[4](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write down the sum:\n",
    "$$\n",
    "y=\\sum_{i=1}^N x_i \\cdot x_i = \\sum_{i=1}^N x_i^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx_i} = 0+ ... + 2x_i + ... + 0 = 2 x_i, \\quad i \\in \\{1,...,N\\}\n",
    "$$\n",
    "\"the shape of the gradient equals the shape of the parameter\" Clark - Computing Neural Network Gradients\n",
    "$$\n",
    "\\frac{dy}{dx} = \\begin{bmatrix} 2x_1  \\\\ \\vdots \\\\ 2x_n \\end {bmatrix} = 2 x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the matrix multiplication in a scalar version:\n",
    "$$\n",
    "[AB]_{ij} = \\sum_{k=1}^N A_{ik} B_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The trace operator applies as in the following way:\n",
    "$$\n",
    "tr(AB) = \\sum_{i=1}^N AB_{ii} =\\sum_{i=1}^N \\sum_{k=1}^N A_{ik} B_{ki}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative with respect to $A_{ik}$ we get:\n",
    "$$\n",
    "\\frac{dy}{dA_{ik}} = B_{ki}\n",
    "$$\n",
    "\n",
    "__OBSERVATION__: This is a sum of sums, which translates to a single sum . In this sum, terms which don't have exactly the term $A$ from row index i and column index k will become 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} = B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "x^TAc=\\sum_{i=1}^N \\sum_{j=1}^N x_i A_{ij} c_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx_i} = \\sum_{j=1}^N A_{ij} c_j = [Ac]_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = Ac\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA_{ij}} = x_i c_j\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} = x c^T\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using Hint__ : We can take the trace of $y$ *(not sure if this solution is correct)* and use cyclic property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "tr(y)=tr(x^TAc)=tr(Acx^T)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the derivative computed at ex. 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} = (cx^T)^T = xc^T\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = ? \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OBS__: The fact is not too obvious as the 2-norm is Jose Nocedal's NO book as:\n",
    "<img src=\"NO.png\">\n",
    "I have found the following readings useful regarding this issue *but was not clarified -  the rabbit hole runs deep* :\n",
    "- Pedersen and Pedersen - The Matrix Cookbook : (262) page 30\n",
    "- [Stackoverflow](https://math.stackexchange.com/questions/546155/proof-that-the-trace-of-a-matrix-is-the-sum-of-its-eigenvalues) and [mathpages](http://www.mathpages.com/home/kmath128.htm) with proofs about the characteristic polynomial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J = || X - AS ||_2^2 = tr([X - AS][X - AS]^T) = tr([X - AS][X^T - S^T A^T]) = tr(X X^T - ASX^T - X S^T A^T + AS S^T A^T) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dS} = - \\frac{\\partial tr(ASX^T)}{\\partial S} - \\frac{\\partial tr(X S^T A^T)}{\\partial S} +\\frac{\\partial tr(AS S^T A^T)}{\\partial S}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference__ : Traa - Matrix Calculus - Notes on the Derivative of a Trace (9) and (12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dS} = - A^TX - A^T X +\\frac{\\partial tr(AS S^T A^T)}{\\partial S} = - 2 A^TX + A^T (S^T A^T)^T + A^T AS = 2(A^T AS - A^T X)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Third approach\n",
    "And finally we can use chain rule! **YOUR TURN** to do it.\n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "$$\n",
    "\\frac{dJ}{dF} =  \n",
    "$$ \n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  \n",
    "$$ \n",
    "(the shape should be $ NM \\times RM )$.\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "J =  || X - F ||_F^2\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J =  \\sum_{i=1}^N \\sum_{j=1}^M ( X_{ij} - F_{ij} )^2 = \\sum_{i=1}^N \\sum_{j=1}^M X_{ij}^2 - 2X_{ij}F_{ij} + F_{ij}^2\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dF_{ij}} = - 2X_{ij} + 2F_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dF} =  2 (F - X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F_{ij}= \\sum_{k=1}^R A_{ik} S_{kj}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dF_{ij}}{dS_{kj}} = A_{ik}  \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OBS__: The rest of the values are 0 . What the above equation says is that the derivative of column $j$ of $F$ by column $j$ from $S$ is $A$ of shape $N \\times R$ for all $j$ in $[1...M]$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chain rule we have that the change in $dJ$ due to $dF$ is multiplied with each scalar intermediate of $dF$ by $dJ$. The index $j$ is already fixed on the left hand side, but the index $i$ is free, and this leads to a summation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference__: Erik Learned-Miller - Vector, Matrix, and Tensor Derivatives section 5, page 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dS_{kj}} =\n",
    "\\sum_{i=1}^N\\frac{dJ}{dF_{ij}} \\frac{dF_{ij}}{dS_{kj}} =\n",
    "\\sum_{i=1}^N (- 2X_{ij} + 2F_{ij} ) A_{ik} = \n",
    "\\sum_{i=1}^N (- 2X_{ij} + 2\\sum_{k=1}^R A_{ik} S_{kj} ) A_{ik} =\n",
    "\\sum_{i=1}^N 2(\\sum_{k=1}^R A_{ik} A_{ik} S_{kj} - A_{ik} X_{ij}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dS_{kj}} =\n",
    "2(\\sum_{i=1}^N\\sum_{k=1}^R A_{ik} A_{ik} S_{kj} - A_{ik} X_{ij}) = \n",
    "2 ([A^T A S]_{kj} - [A^T X]_{kj})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dJ}{dS} = 2 (A^T A S - A^T X)\n",
    "$$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
